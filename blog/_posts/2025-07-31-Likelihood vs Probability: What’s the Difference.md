# Likelihood vs. Probability: Whatâ€™s the Difference (and Why It Matters in Machine Learning)?

> â€œGiven a model, whatâ€™s the chance of this data?â€  
> vs.  
> â€œGiven this data, whatâ€™s the best model?â€

Thatâ€™s the essence of **Probability vs. Likelihood**.

Whether you're diving into machine learning, statistics, or even just curious about coin tosses, understanding the difference between probability and likelihood is foundational â€” yet often misunderstood.

In this blog, weâ€™ll explore both concepts through simple, visual, and intuitive examples â€” with a little help from our old friend: the coin ðŸª™.

---

## Part 1: Understanding Probability

### What Is Probability?

**Probability** is about predicting how likely a certain event is **before** it happens â€” assuming we already know the rules of the system.

### Think Forward:

> â€œIf I know the coin is fair, whatâ€™s the chance of getting 3 heads in a row?â€

This is a **forward problem**. You already know the model â€” now youâ€™re trying to calculate the probability of an outcome.

---

### Example: Fair Coin Toss

Assume the coin is fair:

- \( p = 0.5 \) â†’ Probability of Head
- \( 1 - p = 0.5 \) â†’ Probability of Tail

You toss the coin 3 times.

**What's the chance of getting HHH?**

Since each flip is independent:

\[
P(HHH \mid p=0.5) = 0.5 \times 0.5 \times 0.5 = 0.125
\]

Thatâ€™s a **12.5% chance** â€” only 1 of 8 possible outcomes (since \( 2^3 = 8 \)).

---

## Part 2: Understanding Likelihood

### What Is Likelihood?

**Likelihood** flips the question.

> â€œIâ€™ve seen 3 heads in a row â€” whatâ€™s the most likely value of ( p )?â€

Now, instead of assuming the model and asking about outcomes, you assume the **data** and ask:

> What model (what value of ( p )) makes this data most plausible?

This is the **reverse direction** â€” and itâ€™s the foundation of **Maximum Likelihood Estimation (MLE)**.

---

## Example 1: You Observe HHH (3 Heads)

Letâ€™s say you toss a coin 3 times, and observe: Result = H, H, H

You donâ€™t know if the coin is fair. You want to estimate (p) â€” the probability of Heads â€” that makes this sequence most likely.

### Likelihood Function:

\[
\mathcal{L}(p) = p \cdot p \cdot p = p^3
\]

Why? Because each Head has probability \( p \), and:

\[
\mathcal{L}(p) = P(H) \cdot P(H) \cdot P(H) = p^3
\]

### Maximize It:

\[
\text{Maximize } \mathcal{L}(p) = p^3 \Rightarrow \text{Highest when } p = 1
\]

So:

- **MLE estimate**: \( \hat{p} = 1 \)

In other words, based on HHH, you'd conclude the coin is fully biased toward Heads â€” even though that conclusion is **shaky with such little data**.

*Note: Likelihood doesn't judge; it just finds the best fit for the data youâ€™ve got.*

---

## Example 2: You Observe HHT (2 Heads, 1 Tail)

Now letâ€™s say you observe: Result = H, H, T

This time, the likelihood function becomes:

\[
\mathcal{L}(p) = p^2 (1 - p)
\]

- 2 Heads â†’ \( p^2 \)
- 1 Tail â†’ \( (1 - p) \)

### Evaluate \( \mathcal{L}(p) \) for Different Values:

| \( p \) | \( \mathcal{L}(p) = p^2 (1 - p) \) |
|---------|------------------------------------|
| 0.1     | 0.01 Ã— 0.9 = **0.009**             |
| 0.3     | 0.09 Ã— 0.7 = **0.063**             |
| 0.5     | 0.25 Ã— 0.5 = **0.125**             |
| 0.6     | 0.36 Ã— 0.4 = **0.144**             |
| 0.66    | 0.4356 Ã— 0.34 â‰ˆ **_0.148_**        |
| 0.9     | 0.81 Ã— 0.1 = **0.081**             |

### Result:

The **maximum likelihood occurs at** \( p \approx 0.66 \)

So:

- **MLE estimate**: \( \hat{p} = \frac{2}{3} \)

Why? Because HHT happened, and \( p = 0.66 \) makes that sequence most plausible.

---

## Putting It All Together

| Concept     | Direction     | Assumes     | Solves for         | Example                               |
|-------------|---------------|-------------|--------------------|---------------------------------------|
| Probability | Forward       | Known model | Likelihood of data | Whatâ€™s \( P(HHH) \) if \( p = 0.5 \)? |
| Likelihood  | Reverse (MLE) | Known data  | Best-fitting model | Whatâ€™s best \( p \) if HHH observed?  |

---

## Why This Matters in Machine Learning

In machine learning, especially during model training:

- We assume the **data is fixed** (itâ€™s our training set)
- We optimize the model parameters to **maximize the likelihood**

So every time your model â€œlearns,â€ itâ€™s usually **maximizing a likelihood function**.

---

## ðŸŽ¯ Final Thought

> **Probability predicts. Likelihood explains.**

They may look like twins, but they walk in **opposite directions**.  
Mastering both helps you think clearly about data, models, and uncertainty â€” whether you're flipping coins or training neural networks.

---